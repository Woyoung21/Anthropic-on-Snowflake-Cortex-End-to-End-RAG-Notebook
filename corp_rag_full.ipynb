{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb58124",
   "metadata": {},
   "source": [
    "# ‚ùÑÔ∏è Anthropic on Snowflake Cortex ‚Äì End‚Äëto‚ÄëEnd RAG Notebook\n",
    "\n",
    "This single notebook walks through:\n",
    "\n",
    "1. Setting up a Snowflake database/schema for RAG  \n",
    "2. Ingesting PDF **and** CSV source documents  \n",
    "3. Building a Cortex Search Service  \n",
    "4. Creating a Standards Library table  \n",
    "5. Auto‚Äëclassifying each standard as **Yes / No / Needs Review**  \n",
    "6. (Optional) Launching a Streamlit review dashboard  \n",
    "\n",
    "Cells marked **SQL** are meant to be executed in Snowflake¬†Notebooks with the *SQL* language selector;  \n",
    "cells marked **Python** run in the built‚Äëin Snowpark kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e61e06",
   "metadata": {
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- SECTION¬†0¬†‚Äì¬†Prerequisites  (SQL)\n",
    "USE ROLE ACCOUNTADMIN;            -- or another role with CREATE DB privs\n",
    "CREATE DATABASE IF NOT EXISTS corp_rag;\n",
    "CREATE SCHEMA   IF NOT EXISTS corp_rag;\n",
    "USE DATABASE corp_rag;\n",
    "USE SCHEMA   corp_rag;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e315bdd8",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# SECTION¬†1¬†‚Äì¬†Python environment  (Python)\n",
    "# Make sure the notebook has the packages below via the *Packages* pane:\n",
    "#   snowflake-ml-python>=1.8, snowflake, streamlit\n",
    "\n",
    "import pandas as pd, json, uuid, streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.functions import col\n",
    "from snowflake.cortex import complete\n",
    "from snowflake.core import Root\n",
    "\n",
    "session       = get_active_session()\n",
    "root          = Root(session)\n",
    "warehouse     = session.get_current_warehouse()\n",
    "database_name = session.get_current_database()\n",
    "schema_name   = session.get_current_schema()\n",
    "service_name  = \"document_search_service\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a002d9c",
   "metadata": {
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- SECTION¬†2¬†‚Äì¬†Stage & list source files  (SQL)\n",
    "CREATE STAGE IF NOT EXISTS docs_stage DIRECTORY = (ENABLE = TRUE);\n",
    "-- Drag‚Äëand‚Äëdrop PDFs / CSVs into: Databases ‚ñ∏ corp_rag ‚ñ∏ docs_stage ‚ñ∏ Upload\n",
    "\n",
    "LIST @docs_stage;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057d17c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# SECTION¬†3¬†‚Äì¬†Parse PDFs (Python)\n",
    "from snowflake.snowpark.types import StringType\n",
    "\n",
    "pdf_files = [row[\"name\"].split(\"/\")[1]\n",
    "             for row in session.sql(\"LIST @docs_stage/*.pdf\").collect()]\n",
    "\n",
    "def parse_pdf(file_name: str):\n",
    "    query = \"\"\"\n",
    "        SELECT TO_VARCHAR(\n",
    "            SNOWFLAKE.CORTEX.PARSE_DOCUMENT(@docs_stage, ?, {'mode':'OCR'}):content\n",
    "        ) AS text;\n",
    "    \"\"\"\n",
    "    return (session.sql(query, params=[file_name])\n",
    "                   .collect()[0][\"TEXT\"].replace(\"'\", \"\"))\n",
    "\n",
    "pdf_df = pd.DataFrame({\n",
    "    \"file_name\": pdf_files,\n",
    "    \"text\":      [parse_pdf(f) for f in pdf_files]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d9a2c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# SECTION¬†4¬†‚Äì¬†Load CSVs (Python)\n",
    "session.sql(\"\"\"\n",
    "    CREATE OR REPLACE FILE FORMAT csv_fmt\n",
    "    TYPE = CSV FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1\n",
    "\"\"\").collect()\n",
    "\n",
    "csv_snow = (session.read.schema(\"variant\")\n",
    "                     .option(\"file_format\", \"csv_fmt\")\n",
    "                     .csv(\"@docs_stage/*.csv\"))\n",
    "\n",
    "csv_flat = (csv_snow\n",
    "            .select(col(\"$1\").cast(StringType()).alias(\"text\"),\n",
    "                    col(\"METADATA$FILENAME\").alias(\"file_name\")))\n",
    "\n",
    "csv_df = csv_flat.to_pandas()\n",
    "\n",
    "full_df = pd.concat([pdf_df, csv_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a3b48",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# SECTION¬†5¬†‚Äì¬†Persist docs_text_table (Python)\n",
    "(session.create_dataframe(full_df)\n",
    "        .select(col(\"file_name\"), col(\"text\"))\n",
    "        .write.mode(\"overwrite\")\n",
    "        .save_as_table(\"docs_text_table\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98905a2f",
   "metadata": {
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- SECTION¬†6¬†‚Äì¬†Create / refresh Cortex Search Service  (SQL)\n",
    "CREATE OR REPLACE CORTEX SEARCH SERVICE document_search_service\n",
    "  ON text\n",
    "  ATTRIBUTES file_name\n",
    "  WAREHOUSE = ${warehouse}\n",
    "  TARGET_LAG = '1 day'\n",
    "  EMBEDDING_MODEL = 'snowflake-arctic-embed-l-v2.0'\n",
    "  AS (\n",
    "    SELECT text, file_name FROM docs_text_table\n",
    "  );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5a05c3",
   "metadata": {
    "language": "sql"
   },
   "outputs": [],
   "source": [
    "-- SECTION¬†7¬†‚Äì¬†Standards Library skeleton  (SQL)\n",
    "CREATE OR REPLACE TABLE standards (\n",
    "  id        INT AUTOINCREMENT,\n",
    "  standard  STRING,\n",
    "  answer    STRING,\n",
    "  rationale STRING\n",
    ");\n",
    "\n",
    "INSERT INTO standards (standard) VALUES\n",
    " ('Does the report include an ISO‚Äë27001 certificate?'),\n",
    " ('Is PII encrypted at rest?'),\n",
    " ('Is SOC¬†2¬†Type¬†II coverage current?');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9fb52b",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# SECTION¬†8¬†‚Äì¬†Classify each standard (Python)\n",
    "MODEL = \"claude-3-5-sonnet\"\n",
    "svc = (root.databases[database_name]\n",
    "           .schemas[schema_name]\n",
    "           .cortex_search_services[service_name])\n",
    "\n",
    "def classify(row):\n",
    "    req = row.STANDARD\n",
    "    ctx = (svc.search(query=req, columns=[\"text\"], limit=6)\n",
    "              .to_pandas()[\"text\"].str.cat(sep=\"\\n\\n\"))\n",
    "    prompt = f\"\"\"You are an auditor. Decide if <context> shows <requirement> is met.\n",
    "Return JSON {{\\\"answer\\\":\\\"Yes|No|Needs Review\\\",\\\"rationale\\\":\"one line\"}}.\n",
    "<requirement>{req}</requirement>\n",
    "<context>{ctx}</context>\"\"\"\n",
    "    j = json.loads(complete(MODEL, prompt, temperature=0))\n",
    "    return (row.ID, j[\"answer\"], j[\"rationale\"])\n",
    "\n",
    "std_pdf = session.table(\"standards\").to_pandas()\n",
    "updates = pd.DataFrame([classify(r) for _, r in std_pdf.iterrows()],\n",
    "                       columns=[\"ID\",\"ANSWER\",\"RATIONALE\"])\n",
    "\n",
    "session.write_pandas(updates, \"tmp_updates\", auto_create_table=True)\n",
    "\n",
    "session.sql(\"\"\"MERGE INTO standards t USING tmp_updates s ON s.ID=t.ID\n",
    "              WHEN MATCHED THEN UPDATE SET answer=s.ANSWER, rationale=s.RATIONALE\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eba318",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# SECTION¬†9¬†‚Äì¬†Streamlit review app (Python)\n",
    "# Save the following as streamlit/review_app.py if you want a dashboard:\n",
    "review_code = \"\"\"import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "df = session.table('standards').filter(\"answer='Needs Review'\").to_pandas()\n",
    "st.title('üïµÔ∏è Needs‚ÄëReview Dashboard')\n",
    "edited = st.data_editor(df, num_rows='dynamic',\n",
    "     column_config={'answer': st.column_config.Selectbox('answer', options=['Yes','No','Needs Review'])})\n",
    "if st.button('Save changes'):\n",
    "    session.write_pandas(edited, 'tmp_review', auto_create_table=True)\n",
    "    session.sql('MERGE INTO standards t USING tmp_review s ON s.ID=t.ID WHEN MATCHED THEN UPDATE SET answer=s.answer, rationale=s.rationale').collect()\n",
    "    st.success('Updates saved!')\"\"\"\n",
    "print(review_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30da207",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Save notebook metadata\n",
    "print(\"Notebook cells populated. Ready to run!\")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
